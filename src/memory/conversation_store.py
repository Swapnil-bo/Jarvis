"""
J.A.R.V.I.S. Conversation Store
==================================
Stores every user↔Jarvis exchange as a searchable vector in ChromaDB.

Each exchange is stored as:
  - Document: "User: {question}\nJarvis: {response}"
  - Metadata: timestamp, turn_number
  - Embedding: Auto-generated by ChromaDB's built-in ONNX model
    (all-MiniLM-L6-v2, ~80MB, no PyTorch required)

Retrieval:
  - Semantic search: "What did I ask about cooking?" finds relevant past exchanges
    even if the exact word "cooking" wasn't used.
  - Recency bias: Recent conversations are weighted slightly higher.

Storage: Persistent on disk at ~/.jarvis/memory/
  Survives app restarts and system reboots.
"""

import os
import time
from typing import Optional

import chromadb

from src.utils.config import load_config
from src.utils.logger import get_logger

logger = get_logger("memory.conversation")


class ConversationStore:
    """
    Persistent, searchable conversation history using ChromaDB.
    """

    def __init__(self):
        config = load_config()
        mem_cfg = config["memory"]
        conv_cfg = mem_cfg["conversation"]

        # Expand ~ to full home path
        storage_dir = os.path.expanduser(mem_cfg["storage_dir"])
        os.makedirs(storage_dir, exist_ok=True)

        self.max_results: int = conv_cfg["max_results"]
        self.min_relevance: float = conv_cfg["min_relevance_score"]
        self.max_history: int = conv_cfg["max_history"]
        self.turn_counter: int = 0

        # Initialize ChromaDB with persistent storage
        # Uses built-in ONNX embedding (all-MiniLM-L6-v2, ~80MB)
        # NO PyTorch required — this is critical for 8GB RAM
        self.client = chromadb.PersistentClient(path=storage_dir)

        # Get or create the conversations collection
        self.collection = self.client.get_or_create_collection(
            name=conv_cfg["collection_name"],
            metadata={"hnsw:space": "cosine"},  # Cosine similarity for text
        )

        # Load existing turn count so IDs are continuous across restarts
        self.turn_counter = self.collection.count()

        logger.info(
            f"ConversationStore ready: {self.turn_counter} past exchanges loaded, "
            f"storage={storage_dir}"
        )

    def save_exchange(self, user_text: str, jarvis_response: str) -> None:
        """
        Save a user↔Jarvis exchange to the store.

        Args:
            user_text: What the user said.
            jarvis_response: What Jarvis replied.
        """
        self.turn_counter += 1
        exchange_id = f"turn_{self.turn_counter}"

        # Combine into a single searchable document
        document = f"User: {user_text}\nJarvis: {jarvis_response}"

        self.collection.add(
            ids=[exchange_id],
            documents=[document],
            metadatas=[{
                "timestamp": time.time(),
                "turn_number": self.turn_counter,
                "user_text": user_text,
                "jarvis_response": jarvis_response,
            }],
        )

        logger.debug(
            f"  Saved exchange #{self.turn_counter}: \"{user_text[:50]}...\""
        )

        # Prune oldest if over limit
        if self.turn_counter > self.max_history:
            self._prune_oldest()

    def search(self, query: str, n_results: Optional[int] = None) -> list[dict]:
        """
        Search past conversations by semantic similarity.

        Args:
            query: The current user query to find relevant context for.
            n_results: Number of results (defaults to config max_results).

        Returns:
            List of dicts with keys: document, user_text, jarvis_response,
            timestamp, relevance_score. Sorted by relevance (best first).
        """
        if self.collection.count() == 0:
            return []

        n = n_results or self.max_results

        # Don't request more results than we have
        n = min(n, self.collection.count())

        results = self.collection.query(
            query_texts=[query],
            n_results=n,
            include=["documents", "metadatas", "distances"],
        )

        exchanges = []
        if results and results["documents"] and results["documents"][0]:
            for i, doc in enumerate(results["documents"][0]):
                # ChromaDB returns cosine distance (0 = identical, 2 = opposite)
                # Convert to similarity score (1 = identical, 0 = unrelated)
                distance = results["distances"][0][i]
                similarity = 1.0 - (distance / 2.0)

                if similarity < self.min_relevance:
                    continue

                metadata = results["metadatas"][0][i]
                exchanges.append({
                    "document": doc,
                    "user_text": metadata.get("user_text", ""),
                    "jarvis_response": metadata.get("jarvis_response", ""),
                    "timestamp": metadata.get("timestamp", 0),
                    "turn_number": metadata.get("turn_number", 0),
                    "relevance_score": round(similarity, 3),
                })

        return exchanges

    def _prune_oldest(self):
        """Remove the oldest exchanges to stay under max_history."""
        excess = self.collection.count() - self.max_history
        if excess <= 0:
            return

        # Get oldest turn IDs
        oldest_ids = [f"turn_{i}" for i in range(1, excess + 1)]

        try:
            self.collection.delete(ids=oldest_ids)
            logger.debug(f"  Pruned {len(oldest_ids)} old exchanges")
        except Exception as e:
            logger.warning(f"  Prune failed (non-critical): {e}")

    def get_recent(self, n: int = 5) -> list[dict]:
        """
        Get the N most recent exchanges (for short-term context).

        Returns:
            List of dicts with user_text and jarvis_response, oldest first.
        """
        if self.collection.count() == 0:
            return []

        n = min(n, self.collection.count())

        # Get the most recent turn IDs
        start = max(1, self.turn_counter - n + 1)
        recent_ids = [f"turn_{i}" for i in range(start, self.turn_counter + 1)]

        try:
            results = self.collection.get(
                ids=recent_ids,
                include=["metadatas"],
            )

            exchanges = []
            if results and results["metadatas"]:
                for metadata in results["metadatas"]:
                    exchanges.append({
                        "user_text": metadata.get("user_text", ""),
                        "jarvis_response": metadata.get("jarvis_response", ""),
                        "turn_number": metadata.get("turn_number", 0),
                    })

            # Sort by turn number (oldest first)
            exchanges.sort(key=lambda x: x["turn_number"])
            return exchanges

        except Exception:
            return []
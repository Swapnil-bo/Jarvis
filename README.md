# ü§ñ J.A.R.V.I.S. ‚Äî MacBook Air M1 Edition

> A fully local, voice-activated AI assistant running 100% on Apple Silicon.
> No cloud. No paid APIs. No internet required at runtime.
> Built on a $999 laptop with 8GB RAM.



---

## üé¨ Demo

> Say **"Hey Jarvis"** ‚Üí Ask anything ‚Üí Get a spoken response ‚Äî all running locally.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                 ‚îÇ
‚îÇ    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó      ‚îÇ
‚îÇ    ‚ïë                                      ‚ïë      ‚îÇ
‚îÇ    ‚ïë    ‚ñë‚ñà ‚ñë‚ñà‚ñÄ‚ñà ‚ñë‚ñà‚ñÄ‚ñÑ ‚ñë‚ñà  ‚ñë‚ñà ‚ñë‚ñà ‚ñë‚ñà‚ñÄ‚ñÄ     ‚ïë      ‚îÇ
‚îÇ    ‚ïë    ‚ñë‚ñà ‚ñë‚ñà‚ñÄ‚ñà ‚ñë‚ñà‚ñÄ‚ñÑ ‚ñë‚ñÄ‚ñÑ‚ñÄ  ‚ñë‚ñà ‚ñë‚ñÄ‚ñÄ‚ñà      ‚ïë      ‚îÇ
‚îÇ    ‚ïë    ‚ñà‚ñÑ ‚ñë‚ñà ‚ñà ‚ñë‚ñà ‚ñà  ‚ñë‚ñà   ‚ñë‚ñà ‚ñë‚ñÄ‚ñÄ‚ñÄ       ‚ïë      ‚îÇ
‚îÇ    ‚ïë                                      ‚ïë      ‚îÇ
‚îÇ    ‚ïë    MacBook Air M1 Edition            ‚ïë      ‚îÇ
‚îÇ    ‚ïë    100% Local ‚Ä¢ Zero Cost            ‚ïë      ‚îÇ
‚îÇ    ‚ïë                                      ‚ïë      ‚îÇ
‚îÇ    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù      ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ  ‚úÖ All systems online                          ‚îÇ
‚îÇ  üéôÔ∏è  Say 'Hey Jarvis' to activate               ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ  üîä Wake word detected! Heard: "hey jarvis"     ‚îÇ
‚îÇ  üéØ WAKE WORD TRIGGERED                         ‚îÇ
‚îÇ  üéôÔ∏è  Listening... (speak now)                    ‚îÇ
‚îÇ  üìù Transcription: "What can you do for me?"    ‚îÇ
‚îÇ  üß† Thinking...                                 ‚îÇ
‚îÇ  ü§ñ Jarvis: "I can help with questions,         ‚îÇ
‚îÇ     set reminders, search the web..."           ‚îÇ
‚îÇ  üîä Speaking...                                 ‚îÇ
‚îÇ  üü¢ RAM: 73.4% ‚Äî Cycle 1 complete               ‚îÇ
‚îÇ                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üéØ What This Is

J.A.R.V.I.S. is a personal AI assistant that runs **entirely on a MacBook Air M1 with 8GB RAM**. Every component ‚Äî wake word detection, speech recognition, language understanding, and text-to-speech ‚Äî runs locally with aggressive memory optimization.

**This is not a wrapper around ChatGPT.** Every model runs on-device using Apple's Neural Engine and Metal GPU.

---

## üèóÔ∏è Architecture

```
                         üé§ MacBook Air Microphone
                                  ‚îÇ
                                  ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Continuous InputStream   ‚îÇ
                    ‚îÇ   (zero-gap streaming)     ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                                 ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   85Hz High-Pass Filter    ‚îÇ  ‚Üê Removes fan/AC hum
                    ‚îÇ   (IIR in audio callback)  ‚îÇ     before any processing
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                                 ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Dual Energy Gate         ‚îÇ  ‚Üê avg RMS > 15
                    ‚îÇ   (anti-hallucination)     ‚îÇ     AND peak RMS > 80
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ speech detected
                                 ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Wake Word Detection      ‚îÇ  ‚Üê mlx-whisper base (~140MB)
                    ‚îÇ   "Hey Jarvis" / variants  ‚îÇ     2.5s sliding windows
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ triggered
                                 ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Speech Recording         ‚îÇ  ‚Üê Same stream, zero gaps
                    ‚îÇ   (VAD + min 2s capture)   ‚îÇ     High-pass pre-filtered
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ audio captured
                                 ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Speech-to-Text           ‚îÇ  ‚Üê mlx-whisper small (~240MB)
                    ‚îÇ   (anti-hallucination)     ‚îÇ     Apple Neural Engine
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ text
                                 ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   NLU / Brain              ‚îÇ  ‚Üê Phi-3 Mini 3.8B via Ollama
                    ‚îÇ   (fault-isolated process) ‚îÇ     Metal GPU, Q4, ctx=2048
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ response
                                 ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Text-to-Speech           ‚îÇ  ‚Üê macOS native `say`
                    ‚îÇ   (zero RAM overhead)      ‚îÇ     Voice: Daniel
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üíæ Memory Budget

Running on **8GB unified memory** ‚Äî every megabyte is a conscious decision:

| Component | RAM Usage | Device | Why This Choice |
|-----------|-----------|--------|-----------------|
| Python + deps | ~200MB | CPU | Minimal dependency footprint |
| Wake word (whisper-base) | ~140MB | Neural Engine | Tiny was too inaccurate, base is the sweet spot |
| STT (whisper-small) | ~240MB | Neural Engine | Best accuracy-to-size ratio for transcription |
| Phi-3 Mini (Ollama) | ~2.3GB | Metal GPU | Separate process ‚Äî fault isolation by design |
| High-pass filter | ~0MB | CPU | Pure math on existing arrays |
| macOS TTS | ~0MB | System | Native `say` command, no model to load |
| **Python process total** | **~580MB** | | |
| **System total (peak)** | **~5.4GB / 8GB** | | **27% headroom** ‚úÖ |

---

## üõ°Ô∏è Engineering Decisions

### Why Ollama Stays Separate (Not mlx-lm)
On 8GB, process isolation is a **feature**. If Phi-3 OOMs during a complex query, only the Ollama process dies ‚Äî the audio stream, wake word listener, and Python app survive and can retry. Loading the LLM in-process (via mlx-lm) would mean one memory spike kills everything. This is the same architecture Apple uses for Siri ‚Äî separate daemons per subsystem.

### Why Not Async/Multiprocessing
M1 has one Metal GPU shared across all processes. Whisper and Phi-3 can't run in parallel ‚Äî they'd fight over the same GPU memory. Every pipeline step is either mic-blocked, GPU-blocked, or intentionally blocking (don't listen while speaking). Async adds complexity with zero throughput gain on this hardware.

### Why STT-Based Wake Word (Not openWakeWord)
openWakeWord's embedding model produces **dead inference on M1** ‚Äî max confidence of 0.000017 across 250 audio chunks. The TFLite runtime has no Apple Silicon wheel for Python 3.11, and the ONNX fallback loads but produces zero-confidence predictions. We replaced it with whisper-base doing 2.5-second sliding window transcription with a dual energy gate ‚Äî more accurate and proven to work.

### The "Thank You" Hallucination Fix
Whisper hallucinates "Thank you for watching" and "Subscribe" on near-silence because it feeds its own previous output as context (`condition_on_previous_text=True` by default). Our fix: set it to `False`, tighten `compression_ratio_threshold` to 1.8, and raise `no_speech_threshold` to 0.5. Combined with the 85Hz high-pass filter removing fan noise, hallucinations are eliminated.

### Accent-Aware Trigger Phrases
Whisper-base transcribes "Jarvis" as "Jalvis" with Indian English pronunciation. Rather than fighting the model, we include phonetic variants in the trigger list: `jarvis`, `jalvis`, `hey jarvis`, `hey jalvis`, etc.

---

## üõ†Ô∏è Tech Stack

| Layer | Technology | RAM Cost |
|-------|-----------|----------|
| Audio Streaming | `sounddevice` InputStream + callback queue | ~0MB |
| DSP Filter | 85Hz IIR high-pass (numpy, no scipy) | ~0MB |
| Wake Word | `mlx-whisper` base model | ~140MB |
| Speech-to-Text | `mlx-whisper` small model | ~240MB |
| NLU / Brain | Phi-3 Mini 3.8B via Ollama (Metal GPU) | ~2.3GB |
| Text-to-Speech | macOS native `say` command | ~0MB |
| Memory Profiling | `psutil` + `Rich` logging | ~5MB |
| Config | YAML | ~0MB |

---

## üöÄ Quick Start

### Prerequisites

- macOS with Apple Silicon (M1/M2/M3)
- Python 3.11 (`brew install python@3.11`)
- [Ollama](https://ollama.com) installed and running
- [Homebrew](https://brew.sh)

### Setup

```bash
# Clone
git clone https://github.com/YOUR_USERNAME/jarvis.git
cd jarvis

# System dependencies
brew install portaudio

# AI brain
ollama pull phi3:mini

# Python environment
python3.11 -m venv venv
source venv/bin/activate
pip install -r requirements-phase1.txt

# Launch
python -m src.main
```

### Usage

1. Wait for **"Jarvis is online and ready, sir."**
2. Say **"Hey Jarvis"** at normal conversation volume
3. Wait for **"Yes?"**
4. Ask your question
5. Jarvis responds out loud
6. Repeat

---

## üìÅ Project Structure

```
jarvis/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ jarvis_config.yaml          # All tunable parameters (thresholds, models, prompts)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                     # Entry point ‚Äî voice loop with GC + crash recovery
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ audio.py                # Streaming mic capture + 85Hz high-pass DSP filter
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wake_word.py            # STT-based wake word with dual energy gate
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stt.py                  # Speech-to-text (mlx-whisper, anti-hallucination)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nlu.py                  # Language understanding (Ollama + Phi-3 + fallback)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tts.py                  # Text-to-speech (macOS native)
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py               # YAML config loader
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.py               # Rich logging + psutil RAM monitoring
‚îÇ   ‚îú‚îÄ‚îÄ memory/                     # Phase 2: ChromaDB persistent memory
‚îÇ   ‚îú‚îÄ‚îÄ tools/                      # Phase 3: Mac automation, web search, email
‚îÇ   ‚îú‚îÄ‚îÄ vision/                     # Phase 5: Screen OCR, webcam analysis
‚îÇ   ‚îî‚îÄ‚îÄ ui/                         # Phase 6: Streamlit dashboard
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ diagnose_audio.py           # Audio pipeline diagnostic tool
‚îÇ   ‚îî‚îÄ‚îÄ fix_wake_word.py            # Wake word nuclear troubleshooter
‚îú‚îÄ‚îÄ logs/                           # Runtime logs with memory profiling
‚îú‚îÄ‚îÄ models/                         # Local model weights
‚îú‚îÄ‚îÄ docs/                           # Architecture notes
‚îî‚îÄ‚îÄ requirements-phase1.txt         # Phase 1 Python dependencies
```

---

## üìã Roadmap

- [x] **Phase 1: Voice Core** ‚Äî Wake word ‚Üí STT ‚Üí NLU ‚Üí TTS
  - [x] Continuous streaming audio (zero-gap InputStream)
  - [x] 85Hz high-pass DSP filter
  - [x] Dual-gate wake word detection (avg + peak RMS)
  - [x] Anti-hallucination whisper parameters
  - [x] Fault-isolated NLU via Ollama
  - [x] Mic disconnect auto-recovery
  - [x] GC optimization for 8GB RAM baseline
- [ ] **Phase 2: Memory & Context** ‚Äî ChromaDB persistent memory
- [ ] **Phase 3: Tools & Actions** ‚Äî Mac control, web search, emails, reminders
- [ ] **Phase 4: Code Writing** ‚Äî Autocoding, execution, error loop
- [ ] **Phase 5: Vision** ‚Äî Screen OCR and webcam analysis
- [ ] **Phase 6: Dashboard UI** ‚Äî Streamlit command center

---

## üîß Debugging Journey

Building a local AI assistant on 8GB taught me things no tutorial covers:

1. **openWakeWord is broken on M1** ‚Äî TFLite has no Apple Silicon wheel for Python 3.11. ONNX fallback loads but returns zero confidence. Pivoted to STT-based detection.

2. **`sd.rec()` in a loop drops 40% of audio** ‚Äî Each call has a ~15ms gap while Python runs logic. Over 3 seconds, "Who are you?" becomes garbled and whisper hears "Thank you." Fixed with persistent `InputStream` + callback queue.

3. **Silence threshold is hardware-specific** ‚Äî Started at 500, then 100, finally 30. The MacBook Air M1 mic has very low baseline RMS (~2-7). Every deployment needs threshold tuning.

4. **Whisper hallucinates on silence** ‚Äî Produces "Thank you for watching" from fan noise because `condition_on_previous_text=True` creates feedback loops. Setting it to `False` is mandatory for voice assistants.

5. **Accent handling beats model upgrades** ‚Äî Instead of downloading a larger model, adding "jalvis" to trigger phrases solved recognition instantly.

6. **Process isolation > in-process performance** ‚Äî On constrained hardware, keeping the LLM in a separate process (Ollama) is safer than loading everything into one Python process. Crash isolation matters more than 50ms of IPC overhead.

---

## üí° What Makes This Different

Most "AI assistant" projects are thin wrappers around OpenAI's API. This one:

- **Runs 100% offline** after initial model downloads
- **Costs $0** to run ‚Äî no API keys, no subscriptions, no cloud
- **Respects privacy** ‚Äî your voice never leaves your machine
- **Engineered for constraints** ‚Äî every component chosen to fit in 8GB
- **Production-grade audio** ‚Äî DSP filtering, streaming capture, anti-hallucination
- **Actually documented** ‚Äî every design decision explained with the "why"

---

## ü§ù About

Built by **Swapnil Hazra** (@Swapnil-bo) as a portfolio project for AI Product Management.

This project proves that useful AI doesn't need a datacenter ‚Äî it can run on a $999 laptop with zero ongoing costs.

‚≠ê **Star this repo** if you think local AI is the future.

---

*Built with ‚ù§Ô∏è on Apple Silicon. Part of the 100 Days of Vibe Coding challenge.*
